{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9510107,"sourceType":"datasetVersion","datasetId":5788699},{"sourceId":9517866,"sourceType":"datasetVersion","datasetId":5794619}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-09T15:31:38.347219Z","iopub.execute_input":"2024-11-09T15:31:38.347694Z","iopub.status.idle":"2024-11-09T15:31:38.945655Z","shell.execute_reply.started":"2024-11-09T15:31:38.347649Z","shell.execute_reply":"2024-11-09T15:31:38.944152Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/llmlnknk/train modified/farend_scaled2.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled7.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled18.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled3.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled6.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled23.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled5.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled17.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled11.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled8.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled4.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled16.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled25.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled13.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled24.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled22.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled10.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled20.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled21.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled14.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled9.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled19.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled12.wav\n/kaggle/input/llmlnknk/train modified/farend_scaled15.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled2.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled7.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled18.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled34.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled3.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled6.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled38.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled1.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled23.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled5.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled45.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled36.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled17.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled49.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled11.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled26.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled30.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled48.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled33.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled43.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled32.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled40.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled8.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled4.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled37.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled16.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled44.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled25.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled31.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled42.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled13.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled24.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled27.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled22.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled47.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled50.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled10.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled20.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled46.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled21.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled14.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled29.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled9.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled41.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled19.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled39.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled12.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled35.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled28.wav\n/kaggle/input/modifieddata/dummydtata/farend speech/farend_scaled15.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled15.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled48.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled41.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled23.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled46.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled47.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled34.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled39.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled50.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled17.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled21.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled13.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled42.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled28.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled35.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled2.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled31.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled49.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled30.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled3.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled12.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled45.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled10.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled32.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled38.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled11.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled25.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled8.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled18.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled24.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled22.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled37.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled26.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled4.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled6.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled27.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled16.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled43.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled36.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled33.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled5.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled19.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled29.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled9.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled44.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled7.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled40.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled20.wav\n/kaggle/input/modifieddata/dummydtata/echo_signal/echo_scaled14.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled23.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled9.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled15.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled19.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled47.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled8.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled34.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled41.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled33.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled12.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled4.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled29.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled38.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled45.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled49.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled36.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled21.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled28.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled16.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled7.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled20.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled14.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled24.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled5.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled11.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled32.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled3.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled35.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled43.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled48.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled1.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled6.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled2.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled18.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled10.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled44.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled40.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled39.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled26.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled27.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled17.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled42.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled22.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled31.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled46.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled37.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled13.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled50.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled25.wav\n/kaggle/input/modifieddata/dummydtata/nearendspeech/nearend_scaled30.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled9.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled10.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled24.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled29.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled21.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled36.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled26.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled7.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled31.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled48.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled46.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled28.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled40.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled27.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled4.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled33.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled20.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled8.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled11.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled50.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled30.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled23.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled12.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled43.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled39.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled32.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled14.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled19.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled3.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled18.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled15.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled25.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled22.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled49.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled37.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled1.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled41.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled42.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled6.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled16.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled34.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled35.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled45.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled2.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled5.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled47.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled17.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled44.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled38.wav\n/kaggle/input/modifieddata/dummydtata/nearend_mic_signal/mixed_scaled13.wav\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport torch.nn.functional as F\n\n# Check if a GPU is available and set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Custom dataset\nclass InMemoryDataset(Dataset):\n    def __init__(self, farend_speech, nearend_mic_signal, nearend_speech):\n        self.farend_speech = farend_speech\n        self.nearend_mic_signal = nearend_mic_signal\n        self.nearend_speech = nearend_speech\n\n    def __len__(self):\n        return len(self.farend_speech)\n\n    def __getitem__(self, idx):\n        return (self.nearend_mic_signal[idx], self.nearend_speech[idx])\n\n# Squeeze-and-Excitation (SE) Block\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=8):  # Adjusted reduction for stronger attention\n        super(SEBlock, self).__init__()\n        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, channels, _, _ = x.size()\n        y = F.adaptive_avg_pool2d(x, 1).view(batch_size, channels)\n\n        y = self.fc1(y)\n        y = self.relu(y)\n        y = self.fc2(y)\n        y = self.sigmoid(y)\n\n        y = y.view(batch_size, channels, 1, 1)\n        return x * y.expand_as(x)\n\n# Enhanced Model with SE, Multihead Attention, and LSTM\nclass EnhancedAttentionLSTMModel(nn.Module):\n    def __init__(self):\n        super(EnhancedAttentionLSTMModel, self).__init__()\n        \n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # First convolutional layer\n        self.se1 = SEBlock(16)  # SE Block after conv1\n\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # Second convolutional layer\n        self.se2 = SEBlock(32)  # SE Block after conv2\n        \n        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer\n        \n        # Increase the number of LSTM layers and hidden units\n        self.lstm = nn.LSTM(input_size=32 * 16 * 16, hidden_size=256, num_layers=3, batch_first=True, bidirectional=True)\n        \n        # Multihead Attention layer\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=256 * 2, num_heads=4)  # Bidirectional LSTM has hidden_size*2\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(256 * 2, 128)  # LSTM output to FC1\n        self.fc2 = nn.Linear(128, 1)  # Output layer\n        self.dropout = nn.Dropout(0.3)  # Dropout for regularization\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # Apply conv1 and pool\n        x = self.se1(x)  # Apply SE Block after conv1\n        \n        x = self.pool(F.relu(self.conv2(x)))  # Apply conv2 and pool\n        x = self.se2(x)  # Apply SE Block after conv2\n\n        # Flatten the output and prepare for LSTM\n        batch_size = x.size(0)\n        x = x.view(batch_size, -1)  # Flatten the feature maps\n\n        # Prepare input for LSTM by adding sequence dimension\n        x = x.unsqueeze(1)  # Shape becomes (batch_size, 1, features)\n\n        # LSTM layer\n        lstm_out, _ = self.lstm(x)  # LSTM output\n        \n        # Multihead Attention: Apply attention mechanism to focus on important timesteps\n        attn_output, _ = self.multihead_attn(lstm_out, lstm_out, lstm_out)  # Self-attention\n\n        # Apply fully connected layers with dropout\n        x = self.dropout(F.relu(self.fc1(attn_output[:, 0, :])))  # Use only the first token for final classification\n        \n        return self.fc2(x)  # Output layer\n\n# Function to calculate power\ndef calculate_power(signal):\n    return torch.mean(signal ** 2)\n\n# Normalize the dataset\ndef normalize(data):\n    mean = np.mean(data)\n    std = np.std(data)\n    return (data - mean) / std\n\n# Sample data for demonstration purposes\nnum_samples = 1000  # Adjust as necessary\nfarend_speech = normalize(np.random.rand(num_samples, 64, 64).astype(np.float32))\nnearend_mic_signal = normalize(np.random.rand(num_samples, 64, 64).astype(np.float32))\nnearend_speech = normalize(np.random.rand(num_samples, 1).astype(np.float32))\n\n# Create dataset and dataloader\ndataset = InMemoryDataset(farend_speech, nearend_mic_signal, nearend_speech)\ntrain_loader = DataLoader(dataset, batch_size=16, shuffle=True)  # Smaller batch size for better convergence\n\n# Instantiate model, define loss and optimizer\nmodel = EnhancedAttentionLSTMModel().to(device)\ncriterion = nn.MSELoss()  # Use appropriate loss function\noptimizer = optim.Adam(model.parameters(), lr=0.00005)  # Adjusted learning rate for better performance\n\n# Training loop\nnum_epochs = 200  # Increase the number of epochs for better convergence\ntotal_power_input = 0\ntotal_power_output = 0\ntotal_samples = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    \n    for nearend_mic_mag, nearend_speech_mag in train_loader:\n        nearend_mic_mag = nearend_mic_mag.to(device).unsqueeze(1)  # Add channel dimension\n        nearend_speech_mag = nearend_speech_mag.to(device).unsqueeze(1)  # Shape should be (batch_size, 1)\n\n        optimizer.zero_grad()\n        output = model(nearend_mic_mag)\n\n        # Calculate power for the batch\n        power_input = calculate_power(nearend_mic_mag)\n        power_output = calculate_power(output)\n\n        # Accumulate powers\n        total_power_input += power_input.item() * nearend_mic_mag.size(0)  # Scale by batch size\n        total_power_output += power_output.item() * nearend_mic_mag.size(0)  # Scale by batch size\n        total_samples += nearend_mic_mag.size(0)\n\n        # Backpropagation\n        loss = criterion(output, nearend_speech_mag)\n        loss.backward()\n        optimizer.step()\n\n# After processing all batches for all epochs, compute the average power for the entire training\naverage_power_input = total_power_input / total_samples\naverage_power_output = total_power_output / total_samples\n\n# Avoid log(0) by adding a small value to power_output\naverage_power_output += 1e-10  \n\n# Convert to tensors before calculating ERLE\naverage_power_input_tensor = torch.tensor(average_power_input, device=device)\naverage_power_output_tensor = torch.tensor(average_power_output, device=device)\n\n# Calculate combined ERLE for the entire dataset\ncombined_erle = 10 * torch.log10(average_power_input_tensor / average_power_output_tensor)\n\n# Print the combined ERLE value\nprint(f'Combined ERLE: {combined_erle.item():.4f} dB')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T04:46:01.490461Z","iopub.execute_input":"2024-11-10T04:46:01.491087Z","iopub.status.idle":"2024-11-10T06:14:12.634598Z","shell.execute_reply.started":"2024-11-10T04:46:01.490989Z","shell.execute_reply":"2024-11-10T06:14:12.633144Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([16, 1, 1])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8, 1, 1])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"name":"stdout","text":"Combined ERLE: 45.0247 dB\n","output_type":"stream"}],"execution_count":1}]}